{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf106985",
   "metadata": {},
   "source": [
    "### DIABETES PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a562a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleyy\\anaconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- Age\n",
      "- BloodPressure\n",
      "- DiabetesPedigreeFunction\n",
      "- Glucose\n",
      "- Insulin\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- AGE\n",
      "- BLOODPRESSURE\n",
      "- DIABETESPEDIGREEFUNCTION\n",
      "- GLUCOSE\n",
      "- INSULIN\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 9 features, but KNeighborsClassifier is expecting 16 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m new_model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoting_clf.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Tahmin\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mnew_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_user\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:368\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoting \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 368\u001b[0m     maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# 'hard' voting\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:409\u001b[0m, in \u001b[0;36mVotingClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    Weighted average probability for each class per sample.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    407\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    408\u001b[0m avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_probas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none\n\u001b[0;32m    410\u001b[0m )\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:384\u001b[0m, in \u001b[0;36mVotingClassifier._collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:384\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:268\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"Return probability estimates for the test data X.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    by lexicographic order.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py:745\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    743\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 9 features, but KNeighborsClassifier is expecting 16 features as input."
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# End-to-End Diabetes Machine Learning Pipeline III\n",
    "################################################\n",
    "\n",
    "\n",
    "# Bir şekildebir pipeline oluşturduğumuzu düşünelim. Yani modelledik, modeli çıkardık, sakladık. İşiimiz bir noktada bitti, bu\n",
    "# model nesnesini mesela şu anda takım arkadaşımıza göndderdik. Model development sürecinden sonrs bir modeli canlı sistemlere \n",
    "# entegre etmek demek o model nesnesini çağırmak demektşr. Bunu kapsamlı şekilde ele aldığımız diğer programlar var. Fakat o \n",
    "# noktalarda da kendi kişisel çalışmalarımızda da çok sinsi bir problem vardır. Saç baş yoldurup bu alan bana göre değil mi \n",
    "# acaba diye düşündüren bir problem vardır. Diyelim ki çalışma bitti bunu birisi ile paylaştık. Çalışma ortamını baştan \n",
    "# başlattığımızda neticesinde madem bu model nesnesini okuyup kullanabiliyoruz, okumaya çalışalım bakalım diye düşünüyoruz.\n",
    "\n",
    "# import işlemleri gerçekleştirdik:\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Veri setini okuduk:\n",
    "df = pd.read_csv(\"datasets/diabetes.csv\")\n",
    "\n",
    "# Diyelim ki yeni hastalar geldi. Problem şu hastanemizde yapay zeka ile diyabet tahmini işi yapıyoruz. Model kurduk, sakladık\n",
    "# her şey başarılı. Yeni kayaıtlar geldi. Diyelim ki csv dosyasına bu kayıtlar düştü. Yüksek ihtimalle bu şekilde olur, bir \n",
    "# database'e düşer. Oradan csv olarak çıkarılabilir. Ya da bir uygulamaya düşer. Daha sonra bir database tablosuna düşer ve \n",
    "# oradan çekilir. Yani bir şekilde bunların yeni kayıtları barındırdığını düşünelim. \n",
    "\n",
    "# Buradan rastgele bir hasta çekelim:\n",
    "random_user = df.sample(1, random_state=45)\n",
    "\n",
    "# joblib'den modeli çağıralım:\n",
    "new_model = joblib.load(\"voting_clf.pkl\")\n",
    "\n",
    "# Aşağıdaki şekilde yazdığımızda bir boyut hatası alırız. Bunun sebebi modeli kurarken veri setinin altından girip üstünden \n",
    "# çıktık ve veri setini değiştirdik. Oradaki featurelar artık farklı featurelar, yeni featurelar türettik. Yeni bir kullanıcı,\n",
    "# hasta geldi. İyi de o hastanın bilgileri artık bizim eski yapımıza uygun değil ki, bunlar farklı değişkenler, bizim modeli\n",
    "# kurduğumuz verideki değşkenler farklı değişkenler. Bu veri setinin eski veri setiyle aynı olması dönüştürülmesi lazım. \n",
    "# model deployment süreçlerindeki önemli bariyerlerden problemlerden biris budur. Basit ama en başarılı olabilecek modeli \n",
    "# oluşturma kaygısının da temel sebeplerinden birisi budur. 40 takla atarak oluşturduğumuz bir feature'ın gerçek hayatta \n",
    "# gözlenme freaknsı çok çok düşük olabilir, bir de varsayalım ki feature türetme işlemlerini veri tabanı seviyesinde eda \n",
    "# süreçlerinden sonra yapıyoruz. Nihai bir aggrigated feature tablosu oluşturuyoruz tekilleştirilmiş gözlem birimleri adına.\n",
    "# Dolayısıyla bir de bu şekilde uzun süreçler barındıran bir proje döngüsünde o değişkenlerin üretilmesi veri tabanı yani \n",
    "# sunucular seviyesinde de bir problem meşguliyet işlenmesi de bir problem. Hiç gelmeyecek bir feature'ı da üretemeyip bu \n",
    "# sefer bu pipeline'ın bu boru hattının veri tabanı basamağında problemlerle karşılaşma ihtimali de bir ihtimaldir. \n",
    "# Dolayısıyla burada basitlik vurgusunu yapark bir probleme dikkat çekmeye çalıştık.\n",
    "new_model.predict(random_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9441e86e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diabetes_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Şimdi bu sorunun çözmeye çalışalım. Bu veri setini benzer birşekilde diyabet dataprep modülümüzden (fonksiyonumuzdan) \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# geçirmemiz lazım.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# DİKKAT : Bunu nasıl gerçekleştirbiliriz? from diabetes_pipeline import diabetes_data_prep dersek bizim bu scriptimizin \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# içerisinden sadece bu (diabetes_data_prep) fonksiyonu getiriyor olacak. Şu anda diabetes_data_prep'i çalışmamıza import\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ettik. \u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiabetes_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m diabetes_data_prep\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diabetes_pipeline'"
     ]
    }
   ],
   "source": [
    "# Şimdi bu sorunun çözmeye çalışalım. Bu veri setini benzer bir şekilde diyabet dataprep modülümüzden (fonksiyonumuzdan) \n",
    "# geçirmemiz lazım.\n",
    "\n",
    "# DİKKAT : Bunu nasıl gerçekleştirbiliriz? from diabetes_pipeline import diabetes_data_prep dersek bizim bu scriptimizin \n",
    "# içerisinden sadece bu (diabetes_data_prep) fonksiyonu getiriyor olacak. Şu anda diabetes_data_prep'i çalışmamıza import\n",
    "# ettik. \n",
    "\n",
    "from diabetes_pipeline import diabetes_data_prep\n",
    "\n",
    "# Bu hatayı çözmek için pipeline kısmında bazı fonksiyonları buraya getirmemiz gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd12ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "    \"\"\"\n",
    "\n",
    "    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.\n",
    "    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        dataframe: dataframe\n",
    "                Değişken isimleri alınmak istenilen dataframe\n",
    "        cat_th: int, optional\n",
    "                numerik fakat kategorik olan değişkenler için sınıf eşik değeri\n",
    "        car_th: int, optinal\n",
    "                kategorik fakat kardinal değişkenler için sınıf eşik değeri\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "        cat_cols: list\n",
    "                Kategorik değişken listesi\n",
    "        num_cols: list\n",
    "                Numerik değişken listesi\n",
    "        cat_but_car: list\n",
    "                Kategorik görünümlü kardinal değişken listesi\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "        import seaborn as sns\n",
    "        df = sns.load_dataset(\"iris\")\n",
    "        print(grab_col_names(df))\n",
    "\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "        cat_cols + num_cols + cat_but_car = toplam değişken sayısı\n",
    "        num_but_cat cat_cols'un içerisinde.\n",
    "        Return olan 3 liste toplamı toplam değişken sayısına eşittir: cat_cols + num_cols + cat_but_car = değişken sayısı\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # cat_cols, cat_but_car\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
    "                   dataframe[col].dtypes != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
    "                   dataframe[col].dtypes == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    # num_cols\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "\n",
    "    # print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    # print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    # print(f'cat_cols: {len(cat_cols)}')\n",
    "    # print(f'num_cols: {len(num_cols)}')\n",
    "    # print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    # print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, num_cols, cat_but_car\n",
    "\n",
    "def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n",
    "    quartile1 = dataframe[col_name].quantile(q1)\n",
    "    quartile3 = dataframe[col_name].quantile(q3)\n",
    "    interquantile_range = quartile3 - quartile1\n",
    "    up_limit = quartile3 + 1.5 * interquantile_range\n",
    "    low_limit = quartile1 - 1.5 * interquantile_range\n",
    "    return low_limit, up_limit\n",
    "\n",
    "def replace_with_thresholds(dataframe, variable):\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
    "    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
    "    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n",
    "\n",
    "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
    "    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "    return dataframe\n",
    "\n",
    "def diabetes_data_prep(dataframe):\n",
    "    dataframe.columns = [col.upper() for col in dataframe.columns]\n",
    "\n",
    "    # Glucose\n",
    "    dataframe['NEW_GLUCOSE_CAT'] = pd.cut(x=dataframe['GLUCOSE'], bins=[-1, 139, 200], labels=[\"normal\", \"prediabetes\"])\n",
    "\n",
    "    # Age\n",
    "    dataframe.loc[(dataframe['AGE'] < 35), \"NEW_AGE_CAT\"] = 'young'\n",
    "    dataframe.loc[(dataframe['AGE'] >= 35) & (dataframe['AGE'] <= 55), \"NEW_AGE_CAT\"] = 'middleage'\n",
    "    dataframe.loc[(dataframe['AGE'] > 55), \"NEW_AGE_CAT\"] = 'old'\n",
    "\n",
    "    # BMI\n",
    "    dataframe['NEW_BMI_RANGE'] = pd.cut(x=dataframe['BMI'], bins=[-1, 18.5, 24.9, 29.9, 100],\n",
    "                                        labels=[\"underweight\", \"healty\", \"overweight\", \"obese\"])\n",
    "\n",
    "    # BloodPressure\n",
    "    dataframe['NEW_BLOODPRESSURE'] = pd.cut(x=dataframe['BLOODPRESSURE'], bins=[-1, 79, 89, 123],\n",
    "                                            labels=[\"normal\", \"hs1\", \"hs2\"])\n",
    "\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe, cat_th=5, car_th=20)\n",
    "\n",
    "    cat_cols = [col for col in cat_cols if \"OUTCOME\" not in col]\n",
    "\n",
    "    df = one_hot_encoder(dataframe, cat_cols, drop_first=True)\n",
    "\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(df, cat_th=5, car_th=20)\n",
    "\n",
    "    replace_with_thresholds(df, \"INSULIN\")\n",
    "\n",
    "    X_scaled = StandardScaler().fit_transform(df[num_cols])\n",
    "    df[num_cols] = pd.DataFrame(X_scaled, columns=df[num_cols].columns)\n",
    "\n",
    "    y = df[\"OUTCOME\"]\n",
    "    X = df.drop([\"OUTCOME\"], axis=1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce67e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diabetes_pipeline import diabetes_data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f694972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x ve y'yi çıkaralım. Yeni hastalar veya müşteriler geldi, yeni bir veri var. Bu veriyi prediction işlemi için aynı script'ten\n",
    "# geçirmek zorundayız. prediction yerine farklı birçok kaynakta bu bölüm karşımıza acoring olarak da gelebilir. Bu modeli \n",
    "# kullanıp buradan bir şeyler üretmeye prediction ya da scoring işlemi denmektedir. \n",
    "X, y = diabetes_data_prep(df)\n",
    "\n",
    "# 1 tane örnek seçelim:\n",
    "random_user = X.sample(1, random_state=50)\n",
    "\n",
    "# Aynı işlemi tekrarlayalım:\n",
    "new_model = joblib.load(\"voting_clf.pkl\")\n",
    "\n",
    "new_model.predict(random_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41bba9",
   "metadata": {},
   "source": [
    "**Anlaşılacağı üzere modeli tekrar kullanmak istediğimizde bu modelin anladığı ve bildiği bir şema var, o şemaya uygun bir şekilde değişken seti göndermemiz gerekiyor. Bundan dolayı buradaki veri ön işleme işlemini tekrar ele aldık ama bunun yeni veri olduğunu varsaydık.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9ab5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
