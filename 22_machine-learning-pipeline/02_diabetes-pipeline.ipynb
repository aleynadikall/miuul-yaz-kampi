{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0161dfc9",
   "metadata": {},
   "source": [
    "### DIABETES PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf4de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# End-to-End Diabetes Machine Learning Pipeline II\n",
    "################################################\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "################################################\n",
    "# Helper Functions\n",
    "################################################\n",
    "\n",
    "# Bu bölüm karşımıza helpers.py ya da utils.py şeklinde ilgili projenin çalışma dosyasında dışarıdan da denk gelebilir. Yani \n",
    "# burada machine learning içerisinde helper functions adından bir python dosyası olsaydı bunun içerisinde eğer bu fonksiyonlar\n",
    "# olsaydı bunu da bu şekilde okuyabilirdik. Nasıl okurduk mesela from helpers import * ile hepsini okumayı, from helpers import\n",
    "# a/b/c şeklinde okusaydık istediğimiz bir kısmını okurduk. Bunu bu şekilde yapmak ne getiri ne götürür. Kod okunuşu açısından\n",
    "# daha temiz bir görüntü oluşur, utils ya da helpers adından bir isimlendirme ile bunu dışarıdan da buraya getirebiliriz. Biz \n",
    "# herkesin hata almadan kullanabilmesi için aşağıdaki gibi yapmayı tercih ettik. Çünkü dışarıdan çağırdığımızda bazılarımız \n",
    "# uygulayabilecekken bazılarımız uygulayamayacak.\n",
    "\n",
    "# DİKKAT : Orta ileri seviyede data science kültürü oturmuş bir şirkete girdiğimizde proje dosyalarını okuyor olduğumuzda \n",
    "# utils.py gibi bir dosya görebiliriz, helpers.py şeklinde bir dosya görebiliriz. Bunlar içerisinde projenin ana scriptinde yer\n",
    "# kaplamaması için ya da başka scriptler tarafından da kullanılabilecek fonksiyonları barındırdığından dolayı bu şekilde\n",
    "# dışarıdan çağırılabilir dosyalar görebiliriz. Bunların içerisinde kullanacak olduğumuz fonksiyonlar vardır. Oraya koyduğumuz\n",
    "# durumda buradaki görüntü kirliliğinden kurtuluyor oluruz.\n",
    "\n",
    "# utils.py\n",
    "# helpers.py\n",
    "\n",
    "# Data Preprocessing & Feature Engineering\n",
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "    \"\"\"\n",
    "\n",
    "    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.\n",
    "    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        dataframe: dataframe\n",
    "                Değişken isimleri alınmak istenilen dataframe\n",
    "        cat_th: int, optional\n",
    "                numerik fakat kategorik olan değişkenler için sınıf eşik değeri\n",
    "        car_th: int, optinal\n",
    "                kategorik fakat kardinal değişkenler için sınıf eşik değeri\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "        cat_cols: list\n",
    "                Kategorik değişken listesi\n",
    "        num_cols: list\n",
    "                Numerik değişken listesi\n",
    "        cat_but_car: list\n",
    "                Kategorik görünümlü kardinal değişken listesi\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "        import seaborn as sns\n",
    "        df = sns.load_dataset(\"iris\")\n",
    "        print(grab_col_names(df))\n",
    "\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "        cat_cols + num_cols + cat_but_car = toplam değişken sayısı\n",
    "        num_but_cat cat_cols'un içerisinde.\n",
    "        Return olan 3 liste toplamı toplam değişken sayısına eşittir: cat_cols + num_cols + cat_but_car = değişken sayısı\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # cat_cols, cat_but_car\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
    "                   dataframe[col].dtypes != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
    "                   dataframe[col].dtypes == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    # num_cols\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "\n",
    "    # print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    # print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    # print(f'cat_cols: {len(cat_cols)}')\n",
    "    # print(f'num_cols: {len(num_cols)}')\n",
    "    # print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    # print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, num_cols, cat_but_car\n",
    "\n",
    "def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n",
    "    quartile1 = dataframe[col_name].quantile(q1)\n",
    "    quartile3 = dataframe[col_name].quantile(q3)\n",
    "    interquantile_range = quartile3 - quartile1\n",
    "    up_limit = quartile3 + 1.5 * interquantile_range\n",
    "    low_limit = quartile1 - 1.5 * interquantile_range\n",
    "    return low_limit, up_limit\n",
    "\n",
    "def replace_with_thresholds(dataframe, variable):\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
    "    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
    "    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n",
    "\n",
    "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
    "    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "    return dataframe\n",
    "\n",
    "def diabetes_data_prep(dataframe):\n",
    "    dataframe.columns = [col.upper() for col in dataframe.columns]\n",
    "\n",
    "    # Glucose\n",
    "    dataframe['NEW_GLUCOSE_CAT'] = pd.cut(x=dataframe['GLUCOSE'], bins=[-1, 139, 200], labels=[\"normal\", \"prediabetes\"])\n",
    "\n",
    "    # Age\n",
    "    dataframe.loc[(dataframe['AGE'] < 35), \"NEW_AGE_CAT\"] = 'young'\n",
    "    dataframe.loc[(dataframe['AGE'] >= 35) & (dataframe['AGE'] <= 55), \"NEW_AGE_CAT\"] = 'middleage'\n",
    "    dataframe.loc[(dataframe['AGE'] > 55), \"NEW_AGE_CAT\"] = 'old'\n",
    "\n",
    "    # BMI\n",
    "    dataframe['NEW_BMI_RANGE'] = pd.cut(x=dataframe['BMI'], bins=[-1, 18.5, 24.9, 29.9, 100],\n",
    "                                        labels=[\"underweight\", \"healty\", \"overweight\", \"obese\"])\n",
    "\n",
    "    # BloodPressure\n",
    "    dataframe['NEW_BLOODPRESSURE'] = pd.cut(x=dataframe['BLOODPRESSURE'], bins=[-1, 79, 89, 123],\n",
    "                                            labels=[\"normal\", \"hs1\", \"hs2\"])\n",
    "\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe, cat_th=5, car_th=20)\n",
    "\n",
    "    cat_cols = [col for col in cat_cols if \"OUTCOME\" not in col]\n",
    "\n",
    "    df = one_hot_encoder(dataframe, cat_cols, drop_first=True)\n",
    "\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(df, cat_th=5, car_th=20)\n",
    "\n",
    "    replace_with_thresholds(df, \"INSULIN\")\n",
    "\n",
    "    X_scaled = StandardScaler().fit_transform(df[num_cols])\n",
    "    df[num_cols] = pd.DataFrame(X_scaled, columns=df[num_cols].columns)\n",
    "\n",
    "    y = df[\"OUTCOME\"]\n",
    "    X = df.drop([\"OUTCOME\"], axis=1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Base Models\n",
    "def base_models(X, y, scoring=\"roc_auc\"):\n",
    "    print(\"Base Models....\")\n",
    "    classifiers = [('LR', LogisticRegression()),\n",
    "                   ('KNN', KNeighborsClassifier()),\n",
    "                   (\"SVC\", SVC()),\n",
    "                   (\"CART\", DecisionTreeClassifier()),\n",
    "                   (\"RF\", RandomForestClassifier()),\n",
    "                   ('Adaboost', AdaBoostClassifier()),\n",
    "                   ('GBM', GradientBoostingClassifier()),\n",
    "                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "                   ('LightGBM', LGBMClassifier()),\n",
    "                   # ('CatBoost', CatBoostClassifier(verbose=False))\n",
    "                   ]\n",
    "\n",
    "    for name, classifier in classifiers:\n",
    "        cv_results = cross_validate(classifier, X, y, cv=3, scoring=scoring)\n",
    "        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "\n",
    "# config.py\n",
    "# Mesela bu diyabet pipeline projesi kapsamında şöyle bir yaklaşım sergilenebilir. Mesela config.py adında bir konfigürasyon\n",
    "# dosyası olur. Buraya örneğin veri setinin path'i konabilir, örneğin aşağıdaki parametrelerin bu bölümleri konabilir. Yani\n",
    "# bu sözlükler bu config dosyasının içerisine konulabilir. Aslında konfigürasyon dediğimiz nasıl ki makine öğrenmesi \n",
    "# modellerinde hiperparametre dediğimiz ve dışarıdan bizim ayarladığımız ve değişkenlik gösteren bazı şeyler var. Bu şekildeki\n",
    "# bir veri bilimi, pipeline, makine öğrenmesi proje sürecinde de dışarıdan ayarlanabilecek olan şeyleri bir konfigürasyon \n",
    "# dosyasına da koyabiliriz. Bu konfigürasyon dosyasına girerek orada değişiklikler yaparak ondan sonra ara scriptimizi \n",
    "# çalıştırdığımızda o değişikliklere göre ilerlemesini sağlayabiliriz. Bu oldukça önemli bir konudur, tercih edilebilir ancak\n",
    "# biz burada tercih etmiyoruz. Birkaç örnek daha verelim: Mesela verinin okunma kaynağı, modelin yazılacağı kaynak, elde edilen\n",
    "# hiperparametreleri kaydetmek istiyorsak onların kaydedileceği kaynak, pathleri/dizinleri ifade ediyoruz onlar buraya \n",
    "# girilebilir. output_path dşye buraya bir string nesne oluşturulur ve buraya saklanabilir. Aşağıdaki parametrelerin hepsi\n",
    "# oraya girilebilir. Daha sonra nerden okuyacağız bunu? Az önceki okuma yöntemine benzer bir şekilde from kullanarak okunabilir.\n",
    "# Başka örnek vermek gerekirse bir loglama yapılabilir. Yani çeşitli iterasyonlarda elde edilen sonuçlar biz sadece ekrana \n",
    "# bastırdık, bunların bir log'u, geçmişi tutulabilir. \n",
    "\n",
    "knn_params = {\"n_neighbors\": range(2, 50)}\n",
    "\n",
    "cart_params = {'max_depth': range(1, 20),\n",
    "               \"min_samples_split\": range(2, 30)}\n",
    "\n",
    "rf_params = {\"max_depth\": [8, 15, None],\n",
    "             \"max_features\": [5, 7, \"auto\"],\n",
    "             \"min_samples_split\": [15, 20],\n",
    "             \"n_estimators\": [200, 300]}\n",
    "\n",
    "xgboost_params = {\"learning_rate\": [0.1, 0.01],\n",
    "                  \"max_depth\": [5, 8],\n",
    "                  \"n_estimators\": [100, 200],\n",
    "                  \"colsample_bytree\": [0.5, 1]}\n",
    "\n",
    "lightgbm_params = {\"learning_rate\": [0.01, 0.1],\n",
    "                   \"n_estimators\": [300, 500],\n",
    "                   \"colsample_bytree\": [0.7, 1]}\n",
    "\n",
    "classifiers = [('KNN', KNeighborsClassifier(), knn_params),\n",
    "               (\"CART\", DecisionTreeClassifier(), cart_params),\n",
    "               (\"RF\", RandomForestClassifier(), rf_params),\n",
    "               ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),\n",
    "               ('LightGBM', LGBMClassifier(), lightgbm_params)]\n",
    "\n",
    "def hyperparameter_optimization(X, y, cv=3, scoring=\"roc_auc\"):\n",
    "    print(\"Hyperparameter Optimization....\")\n",
    "    best_models = {}\n",
    "    for name, classifier, params in classifiers:\n",
    "        print(f\"########## {name} ##########\")\n",
    "        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}\")\n",
    "\n",
    "        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)\n",
    "        final_model = classifier.set_params(**gs_best.best_params_)\n",
    "\n",
    "        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}\")\n",
    "        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n",
    "        best_models[name] = final_model\n",
    "    return best_models\n",
    "\n",
    "# Stacking & Ensemble Learning\n",
    "def voting_classifier(best_models, X, y):\n",
    "    print(\"Voting Classifier...\")\n",
    "    voting_clf = VotingClassifier(estimators=[('KNN', best_models[\"KNN\"]), ('RF', best_models[\"RF\"]),\n",
    "                                              ('LightGBM', best_models[\"LightGBM\"])],\n",
    "                                  voting='soft').fit(X, y)\n",
    "    cv_results = cross_validate(voting_clf, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "    print(f\"Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "    print(f\"F1Score: {cv_results['test_f1'].mean()}\")\n",
    "    print(f\"ROC_AUC: {cv_results['test_roc_auc'].mean()}\")\n",
    "    return voting_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b221df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Şimdi öyle birişlem yapmamız lazım ki örneğin bu python scriptini bir proje dosyası gibi bir sunucu ortamında ya da kişisel\n",
    "# bir bilgisayarda iişletim sistemi seviyesinden çalıştırabiliyor olmalıyız. Dolayısıyla bu bölümdeki amacımız bu pipeline\n",
    "# sürecini daha iyi anlamaya çalışmak ve süreci tamamlamak.\n",
    "\n",
    "# DİKKAT : Pipeline konusu iş aldıran serisinden çok değerli bir konudur. Online bir eğitimde böyle bir süreci kabul edilebilir\n",
    "# bir kapsayıcılıkta deneyimleyebiliyor olmak oldukça değerlidir. Günün sonunda pipeline dediğimiz şey budur.\n",
    "\n",
    "\n",
    "################################################\n",
    "# Pipeline Main/Run Function\n",
    "################################################\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Burada bir veri okuma işlemi vardır. Bu bir csv dosyasından olmaz da daha çok veri tabanlarından olur. Veri tabanından \n",
    "    # veriyi çek. Pipeline'ın boru hattının ilk aşamasıdır. \n",
    "    df = pd.read_csv(\"datasets/diabetes.csv\")\n",
    "    \n",
    "    # Daha sonra çekmiş olduğumuz veriyi veri ön işleme scriptinden geçiriyoruz. Bu Pipeline'ın 2.aşamasıdır.\n",
    "    X, y = diabetes_data_prep(df)\n",
    "    \n",
    "    # 3.adım genel modellere bakmaktır. Bunu buraya koymayabiliriz de bu kısım daha çok research kısmına özeldir. Ancak bir \n",
    "    # görelim düşüncesiyle buraya koyduk.\n",
    "    base_models(X, y)\n",
    "    \n",
    "    # Bir sonraki adım hiperparametre optimizasyonudur.\n",
    "    best_models = hyperparameter_optimization(X, y)\n",
    "    \n",
    "    # Yukarıdan gelen en iyi modellerle bir voting classifier oluşturuyoruz:\n",
    "    voting_clf = voting_classifier(best_models, X, y)\n",
    "    \n",
    "    # Model nesnesini bir yere kaydediyoruz. Bu şekilde bir train sürecini bir pipeline aracılığıyla tamamlamış olduk. \n",
    "    joblib.dump(voting_clf, \"voting_clf.pkl\")\n",
    "    \n",
    "    return voting_clf\n",
    "# Burada voting_clf kullanmayacağımızdan dolayı return etmemeyi tercih edebiliriz. Biz şu anda bir tarining sürecini planladık. \n",
    "# Yani bir pipeline'ın amacı uçtan uca çeşitli basamaklardaki veri ön işlemeleri ve benzeri görevleri yerine getirerek \n",
    "# classifier nesnesini dışarıya pkl dosyası olarak kaydetmektir. Bu pkl dosyasını daha sonra farklı bir kodun içerisinde \n",
    "# çalıştırabilir, çağırabilir ve kullanabiliriz. Dolayısıyla bu ana görev kapsamında bu training scriptinde bu voting_clf'ı\n",
    "# çalıştırılmayacağından dolayı bunu return etmeyebiliriz de, bu da iyi bir tercih olabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a10a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İşlem başladı\n",
      "Base Models....\n",
      "roc_auc: 0.8409 (LR) \n",
      "roc_auc: 0.791 (KNN) \n",
      "roc_auc: 0.8355 (SVC) \n",
      "roc_auc: 0.658 (CART) \n",
      "roc_auc: 0.8198 (RF) \n",
      "roc_auc: 0.8196 (Adaboost) \n",
      "roc_auc: 0.8263 (GBM) \n",
      "roc_auc: 0.8015 (XGBoost) \n",
      "roc_auc: 0.807 (LightGBM) \n",
      "Hyperparameter Optimization....\n",
      "########## KNN ##########\n",
      "roc_auc (Before): 0.791\n",
      "roc_auc (After): 0.8211\n",
      "KNN best params: {'n_neighbors': 20}\n",
      "\n",
      "########## CART ##########\n",
      "roc_auc (Before): 0.6575\n",
      "roc_auc (After): 0.7943\n",
      "CART best params: {'max_depth': 6, 'min_samples_split': 23}\n",
      "\n",
      "########## RF ##########\n",
      "roc_auc (Before): 0.8301\n",
      "roc_auc (After): 0.834\n",
      "RF best params: {'max_depth': 8, 'max_features': 7, 'min_samples_split': 15, 'n_estimators': 200}\n",
      "\n",
      "########## XGBoost ##########\n",
      "roc_auc (Before): 0.8015\n",
      "roc_auc (After): 0.8255\n",
      "XGBoost best params: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "\n",
      "########## LightGBM ##########\n",
      "roc_auc (Before): 0.807\n",
      "roc_auc (After): 0.8234\n",
      "LightGBM best params: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'n_estimators': 300}\n",
      "\n",
      "Voting Classifier...\n",
      "Accuracy: 0.7669270833333334\n",
      "F1Score: 0.6286367284819298\n",
      "ROC_AUC: 0.8373325908086359\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"İşlem başladı\")\n",
    "    main()\n",
    "\n",
    "# Bunu pycharm üzerinden de yap. O zaman if'in yanında bir play tuşu çıkacak, ona basarak işini halledebilirsin.\n",
    "\n",
    "# git github\n",
    "# makefile\n",
    "# veri tabanlarından\n",
    "# log\n",
    "# class\n",
    "# docker\n",
    "# requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078713db",
   "metadata": {},
   "source": [
    "**DİKKAT :** Yukarıdaki ifadeye dunder name dunder main denir. Yani double under score'dur. Eğer ortasında name varsa dunder name diye, ortasında main varsa dunder main diye okunur. \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "Bloğu der ki eğer çalıştırılabilir bir nesne varsa o zaman bu işi yap der ve main fonksiyonunu çağırır. Yani fonksiyon isimlendirmesinde main yazdığımızdan dolayı ve burada maini sorguluyor gibi gözüktüğümüzden dolayı bu çalışmaz. Herhangi yukarıda import işlemi tanımlama işlemi gibi bazı işlemler varsa bu durumda dunder name dunder main bloğu çalışır ve if __name__ == \"__main__\" kısmı sağlanıyorsa bunu yap ( main() ) dendiğindeki görev burada çalışmaya başlar. \n",
    "\n",
    "**İyi de neden böyle yapıyoruz. Yukarıdaki fonksiyonların hepsini seçip çalıştırırız.**\n",
    "Problem şudur ki bunu komut satırından çalıştırmak istediğimizde kodumuzun içine girip çalıştırması görevini görmesi açısından yukarıdaki şekilde dunder name dunder main olarak yapmalıyız.\n",
    "\n",
    "Bunun temel kullanılma amacı bir script'i bir python dosyasını aslında tetikleyici bölümdür. Bir python dosyasını çalıştıracak olan nihai bölümdür. Biz bunu işletim sistemi seviyesinden bu scripti çalıştırdığımızda tanımlandı tanımlandı ee bu da tanımlandı. Buraya bir kontrol mekanizması koyalım da burası onu çalıştıracak bölüm olsun diye düşünürüz ve bu bölüm aracılığıyla kod çalışır. Bunu yazmasak da çalışır aslında fakat burada execution işlemi başlamadan önce bu nokta bize çeşitli dokunuşlar gerçekleştirme imkanı sağlar. İçine print ile işlem başladı kodunu eklersek. İlk çalışacak kısım dunder name dunder main bloğudur. Doğru ise çalışma başlatılır. main'i başlatmadan önce dolayısıyla print yazdığımız yere bir rapor ekleyebiliriz. Sadece bu mu hayır buraya timer fonksiyonları atayabiliriz. Yani çeşitli görevlerin ne kadar sürdüğünü hesaplama işlemleri yapabiliriz. Mesela cli ekleyebiliriz. Yani comment line interface ekleyebiliriz. Bunları da burada çeşitli etkileşimlere sokabiliriz. Yani eğer işletim sistemi seviyesinden bu kodu çalıştıran bir kişi bir argüman girdiyse yani dışarıdan bu scriptin içerisini biçimlendirmek değiştirmek istiyorsa bu değiştirme biçimlendirme işlemini bu ilk run olacak (dunder name dunder main kısmında) bölümde şekillendirebiliriz. Dolayısıyla bu gibi birçok amaçla kullanılabilir. Özetle bizim aklımızda tutmamız gereken nokta bir python scriptini işletim sistemi seviyesinden yani komut satırı seviyesinden run etmek istediğimizde böyle bir blok ekleyerek gerçekleştirebiliriz olmalıdır. \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": bloğu devreye girecektir. eğer içe aktarılmışsa bu blok çalıştırılmayacak ve __name __, sizin betiğe verdiğiniz isim olarak geçecektir. \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Eğer doğrudan çalıştırılmışsam beni görürsünüz.\")\n",
    "    print(__name__)\n",
    "\n",
    "else:\n",
    "    print(\"Beni yalnızca içe aktardığınızda görebilirsiniz.\")\n",
    "    print(__name__)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f87007",
   "metadata": {},
   "source": [
    "**Konunun devamı bilgisayarındaki pipeline videosunda var.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462cad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
